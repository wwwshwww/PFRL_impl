{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd00e4c6caba1d3762b61e4a30fb3cfa6f47eab88e5344d94b2359cf7b7c0fd8e66",
   "display_name": "Python 3.8.6 64-bit ('.venv': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Soft Actor Critic with env that observe Image\n",
    "Version without VAE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def display_frames_as_gif(frames, filename='CarRacing_SAC.gif'):\n",
    "    frs = [Image.fromarray(f, mode='RGB') for f in frames]\n",
    "    frs[0].save('./result/'+filename, save_all=True, append_images=frs[1:], optimize=False, duration=40, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import distributions, nn\n",
    "import pfrl\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            elif i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "class WrapPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(WrapPyTorch, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = spaces.Box(\n",
    "            self.observation_space.low[0,0,0],\n",
    "            self.observation_space.high[0,0,0],\n",
    "            [obs_shape[2], obs_shape[0], obs_shape[1]],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)\n",
    "\n",
    "class WrapFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, \n",
    "            shape=(self.observation_space.shape[0], self.observation_space.shape[1], 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        return frame[:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/moyash/python_workspace/pfrl_impl/.venv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v0')\n",
    "env = MaxAndSkipEnv(env, skip=4)\n",
    "# env = WrapFrame(env)\n",
    "env = pfrl.wrappers.CastObservationToFloat32(env)\n",
    "env = WrapPyTorch(env)\n",
    "# env = pfrl.wrappers.NormalizeActionSpace(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "timelimit: 1000\nobs_space: Box(0.0, 255.0, (3, 96, 96), float32), \naction_space: Box(-1.0, 1.0, (3,), float32)\nobs_size: 27648, \naction_size: 3\n(3, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "timestep_limit = env.spec.max_episode_steps\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "obs_size = obs_space.low.size\n",
    "action_size = action_space.low.size\n",
    "\n",
    "print(f'timelimit: {timestep_limit}')\n",
    "print(f'obs_space: {obs_space}, \\naction_space: {action_space}')\n",
    "print(f'obs_size: {obs_size}, \\naction_size: {action_size}')\n",
    "print(obs_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "    return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "def make_conv2d_layer(width, height, out_size=50):\n",
    "    convW = conv2d_size_out(width, 5, 2)\n",
    "    convW = conv2d_size_out(convW, 5, 2)\n",
    "    convW = conv2d_size_out(convW, 3, 1)\n",
    "\n",
    "    convH = conv2d_size_out(height, 5, 2)\n",
    "    convH = conv2d_size_out(convH, 5, 2)\n",
    "    convH = conv2d_size_out(convH, 3, 1)\n",
    "\n",
    "    linear_input_size = convW * convH * 32\n",
    "\n",
    "    # RGB Image tensor as input\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=5,stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=5, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3,stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(linear_input_size, out_size),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def make_linear_layer(linear_input_size, out_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(linear_input_size, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, out_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squashed_diagonal_gaussian_head(x):\n",
    "    assert x.shape[-1] == action_size * 2\n",
    "    mean, log_scale = torch.chunk(x, 2, dim=1)\n",
    "    log_scale = torch.clamp(log_scale, -20.0, 2.0)\n",
    "    var = torch.exp(log_scale * 2)\n",
    "    base_distribution = distributions.Independent(\n",
    "        distributions.Normal(loc=mean, scale=torch.sqrt(var)), 1\n",
    "    )\n",
    "    # cache_size=1 is required for numerical stability\n",
    "    return distributions.transformed_distribution.TransformedDistribution(\n",
    "        base_distribution, [distributions.transforms.TanhTransform(cache_size=1)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyFunction(nn.Module):\n",
    "    def __init__(self, width, height, action_size, features_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        # RGB Image tensor as input\n",
    "        self.selectTrackFeatures = make_conv2d_layer(width, height, features_size)\n",
    "        self.fc1 = make_linear_layer(features_size, action_size*2)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.selectTrackFeatures(state)\n",
    "        x = self.fc1(x)\n",
    "        return squashed_diagonal_gaussian_head(x)\n",
    "\n",
    "policy = PolicyFunction(obs_space.shape[1], obs_space.shape[2], action_size)\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)"
   ]
  },
  {
   "source": [
    "# print(obs_space.sample().shape)\n",
    "# policy(torch.from_numpy(obs_space.sample()).unsqueeze(0))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(nn.Module):\n",
    "    def __init__(self, width, height, action_size, features_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        # RGB Image tensor as input\n",
    "        self.selectTrackFeatures = make_conv2d_layer(width, height, features_size)\n",
    "        self.fc1 = make_linear_layer(features_size+action_size, 1)\n",
    "    \n",
    "    def forward(self, state_and_action):\n",
    "        state = self.selectTrackFeatures(state_and_action[0])\n",
    "        x = torch.cat((state, state_and_action[1]), dim=-1)\n",
    "        return self.fc1(x)\n",
    "\n",
    "q_func1 = QFunction(obs_space.shape[1], obs_space.shape[2], action_size)\n",
    "q_func2 = QFunction(obs_space.shape[1], obs_space.shape[2], action_size)\n",
    "q_func1_optimizer = torch.optim.Adam(q_func1.parameters(), lr=3e-4)\n",
    "q_func2_optimizer = torch.optim.Adam(q_func2.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "source": [
    "# obs = torch.from_numpy(obs_space.sample()).unsqueeze(0)\n",
    "# print(obs.shape)\n",
    "# action = torch.from_numpy(action_space.sample()).unsqueeze(0)\n",
    "# print(action.shape)\n",
    "# q_func1((obs, action))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbuf = pfrl.replay_buffers.ReplayBuffer(10 ** 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def burnin_action_func():\n",
    "    \"\"\"Select random actions until model is updated one or more times.\"\"\"\n",
    "    return np.random.uniform(action_space.low, action_space.high).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "replay_start_size = 10000\n",
    "gpu = 0\n",
    "batch_size = 256\n",
    "entropy_target = -action_size\n",
    "temperature_optimizer_lr = 3e-4\n",
    "\n",
    "agent = pfrl.agents.SoftActorCritic(\n",
    "    policy,\n",
    "    q_func1,\n",
    "    q_func2,\n",
    "    policy_optimizer,\n",
    "    q_func1_optimizer,\n",
    "    q_func2_optimizer,\n",
    "    rbuf,\n",
    "    gamma=gamma,\n",
    "    replay_start_size=replay_start_size,\n",
    "    gpu=gpu,\n",
    "    minibatch_size=batch_size,\n",
    "    burnin_action_func=burnin_action_func,\n",
    "    entropy_target=entropy_target,\n",
    "    temperature_optimizer_lr=temperature_optimizer_lr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Track generation: 957..1207 -> 250-tiles track\n",
      "Track generation: 1027..1288 -> 261-tiles track\n",
      "Track generation: 1215..1523 -> 308-tiles track\n",
      "Track generation: 1242..1557 -> 315-tiles track\n",
      "Track generation: 1162..1456 -> 294-tiles track\n",
      "Track generation: 1236..1549 -> 313-tiles track\n",
      "Track generation: 1051..1318 -> 267-tiles track\n",
      "Track generation: 1102..1382 -> 280-tiles track\n",
      "Track generation: 1118..1402 -> 284-tiles track\n",
      "Track generation: 1247..1563 -> 316-tiles track\n",
      "episode: 10 R: -30.158730158730325 \n",
      "statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n",
      "Track generation: 1221..1530 -> 309-tiles track\n",
      "Track generation: 1307..1638 -> 331-tiles track\n",
      "Track generation: 1272..1594 -> 322-tiles track\n",
      "Track generation: 1011..1268 -> 257-tiles track\n",
      "Track generation: 1235..1548 -> 313-tiles track\n",
      "Track generation: 1139..1428 -> 289-tiles track\n",
      "Track generation: 1483..1864 -> 381-tiles track\n",
      "Track generation: 1396..1749 -> 353-tiles track\n",
      "Track generation: 1104..1391 -> 287-tiles track\n",
      "Track generation: 1123..1408 -> 285-tiles track\n",
      "episode: 20 R: -26.05633802816901 \n",
      "statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n",
      "Track generation: 1139..1428 -> 289-tiles track\n",
      "Track generation: 1230..1549 -> 319-tiles track\n",
      "Track generation: 1149..1440 -> 291-tiles track\n",
      "Track generation: 1165..1460 -> 295-tiles track\n",
      "Track generation: 1121..1413 -> 292-tiles track\n",
      "Track generation: 1005..1269 -> 264-tiles track\n",
      "Track generation: 1143..1433 -> 290-tiles track\n",
      "Track generation: 1077..1350 -> 273-tiles track\n",
      "Track generation: 1135..1423 -> 288-tiles track\n",
      "Track generation: 1060..1337 -> 277-tiles track\n",
      "episode: 30 R: -16.66666666666659 \n",
      "statistics: [('average_q1', 2.1280253), ('average_q2', 2.2312496), ('average_q_func1_loss', 0.7618689721538907), ('average_q_func2_loss', 0.6508724419843583), ('n_updates', 21), ('average_entropy', 2.0416327), ('temperature', 0.9936731457710266)]\n",
      "Track generation: 1111..1393 -> 282-tiles track\n",
      "Track generation: 1209..1524 -> 315-tiles track\n",
      "Track generation: 1029..1295 -> 266-tiles track\n",
      "Track generation: 1085..1368 -> 283-tiles track\n",
      "Track generation: 1132..1419 -> 287-tiles track\n",
      "Track generation: 1160..1454 -> 294-tiles track\n",
      "Track generation: 987..1244 -> 257-tiles track\n",
      "Track generation: 1103..1383 -> 280-tiles track\n",
      "Track generation: 1245..1560 -> 315-tiles track\n",
      "Track generation: 1245..1561 -> 316-tiles track\n",
      "episode: 40 R: -11.111111111110823 \n",
      "statistics: [('average_q1', 17.296778), ('average_q2', 17.28575), ('average_q_func1_loss', 0.14063726395368575), ('average_q_func2_loss', 0.15695426374673843), ('n_updates', 3361), ('average_entropy', 2.0414765), ('temperature', 0.4218190908432007)]\n",
      "Track generation: 954..1201 -> 247-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1162..1456 -> 294-tiles track\n",
      "Track generation: 980..1233 -> 253-tiles track\n",
      "Track generation: 1175..1473 -> 298-tiles track\n",
      "Track generation: 1191..1493 -> 302-tiles track\n",
      "Track generation: 1043..1307 -> 264-tiles track\n",
      "Track generation: 1083..1324 -> 241-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1176..1474 -> 298-tiles track\n",
      "Track generation: 1215..1528 -> 313-tiles track\n",
      "Track generation: 1076..1349 -> 273-tiles track\n",
      "Track generation: 1176..1474 -> 298-tiles track\n",
      "Track generation: 1015..1276 -> 261-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1091..1370 -> 279-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1226..1536 -> 310-tiles track\n",
      "episode: 50 R: -80.5825242718443 \n",
      "statistics: [('average_q1', 18.33327), ('average_q2', 18.302593), ('average_q_func1_loss', 0.11124881986528636), ('average_q_func2_loss', 0.1421393310278654), ('n_updates', 6701), ('average_entropy', 1.9645115), ('temperature', 0.1979489028453827)]\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 50\n",
    "max_episode_len = 1000\n",
    "\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while True:\n",
    "        # Uncomment to watch the behavior in a GUI window\n",
    "        # env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        reset = t == max_episode_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        # print(f\"action: {action}, reward: {reward}\")\n",
    "        if done or reset:\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i, 'R:', R, '\\nstatistics:', agent.get_statistics())\n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "source": [
    "### Random"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Track generation: 1093..1377 -> 284-tiles track\n",
      "R: -57.59717314487675\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "frames = []\n",
    "obs = env.reset()\n",
    "total_r = 0\n",
    "\n",
    "while not done:\n",
    "    action = -1+np.random.rand(3)*2\n",
    "    obs, r, done, info = env.step(action)\n",
    "    total_r += r\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "\n",
    "print('R:', total_r)\n",
    "display_frames_as_gif(frames, 'CarRacing_Random.gif')"
   ]
  },
  {
   "source": [
    "### Trained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Track generation: 1048..1319 -> 271-tiles track\n",
      "R: 33.333333333336306\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "frames = []\n",
    "obs = env.reset()\n",
    "total_r = 0\n",
    "\n",
    "with agent.eval_mode():\n",
    "    while not done:\n",
    "        action = agent.act(obs)\n",
    "        obs, r, done, info = env.step(action)\n",
    "        total_r += r\n",
    "        agent.observe(obs, r, done, reset)\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "print('R:', total_r)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}