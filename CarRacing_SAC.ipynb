{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd00e4c6caba1d3762b61e4a30fb3cfa6f47eab88e5344d94b2359cf7b7c0fd8e66",
   "display_name": "Python 3.8.6 64-bit ('.venv': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Soft Actor Critic with env that observe Image\n",
    "Version without VAE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def display_frames_as_gif(frames, filename='CarRacing_SAC.gif'):\n",
    "    frs = [Image.fromarray(f, mode='RGB') for f in frames]\n",
    "    frs[0].save('./result/'+filename, save_all=True, append_images=frs[1:], optimize=False, duration=40, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import distributions, nn\n",
    "import pfrl\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            elif i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "class WrapPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(WrapPyTorch, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = spaces.Box(\n",
    "            self.observation_space.low[0,0,0],\n",
    "            self.observation_space.high[0,0,0],\n",
    "            [obs_shape[2], obs_shape[0], obs_shape[1]],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)\n",
    "\n",
    "class WrapFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, \n",
    "            shape=(self.observation_space.shape[0], self.observation_space.shape[1], 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        return frame[:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/moyash/python_workspace/pfrl_impl/.venv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v0')\n",
    "env = MaxAndSkipEnv(env, skip=5)\n",
    "# env = WrapFrame(env)\n",
    "env = pfrl.wrappers.CastObservationToFloat32(env)\n",
    "env = WrapPyTorch(env)\n",
    "# env = pfrl.wrappers.NormalizeActionSpace(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "timelimit: 1000\nobs_space: Box(0.0, 255.0, (3, 96, 96), float32), \naction_space: Box(-1.0, 1.0, (3,), float32)\nobs_size: 27648, \naction_size: 3\n(3, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "timestep_limit = env.spec.max_episode_steps\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "obs_size = obs_space.low.size\n",
    "action_size = action_space.low.size\n",
    "\n",
    "print(f'timelimit: {timestep_limit}')\n",
    "print(f'obs_space: {obs_space}, \\naction_space: {action_space}')\n",
    "print(f'obs_size: {obs_size}, \\naction_size: {action_size}')\n",
    "print(obs_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "    return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "def make_conv2d_layer(width, height):\n",
    "    convW = conv2d_size_out(width, 5, 2)\n",
    "    convW = conv2d_size_out(convW, 5, 2)\n",
    "    convW = conv2d_size_out(convW, 3, 1)\n",
    "\n",
    "    convH = conv2d_size_out(height, 5, 2)\n",
    "    convH = conv2d_size_out(convH, 5, 2)\n",
    "    convH = conv2d_size_out(convH, 3, 1)\n",
    "\n",
    "    linear_input_size = convW * convH * 64\n",
    "    print(linear_input_size)\n",
    "\n",
    "    # RGB Image tensor as input\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=5,stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=5, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3,stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "    ), linear_input_size\n",
    "\n",
    "def make_linear_layer(linear_input_size, out_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(linear_input_size, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, out_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squashed_diagonal_gaussian_head(x):\n",
    "    assert x.shape[-1] == action_size * 2\n",
    "    mean, log_scale = torch.chunk(x, 2, dim=1)\n",
    "    log_scale = torch.clamp(log_scale, -20.0, 2.0)\n",
    "    var = torch.exp(log_scale * 2)\n",
    "    base_distribution = distributions.Independent(\n",
    "        distributions.Normal(loc=mean, scale=torch.sqrt(var)), 1\n",
    "    )\n",
    "    # cache_size=1 is required for numerical stability\n",
    "    return distributions.transformed_distribution.TransformedDistribution(\n",
    "        base_distribution, [distributions.transforms.TanhTransform(cache_size=1)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23104\n"
     ]
    }
   ],
   "source": [
    "class PolicyFunction(nn.Module):\n",
    "    def __init__(self, width, height, action_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # RGB Image tensor as input\n",
    "        self.selectTrackFeatures, self.linear_input_size = make_conv2d_layer(width, height)\n",
    "        self.fc1 = make_linear_layer(self.linear_input_size, action_size*2)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.selectTrackFeatures(state)\n",
    "        x = self.fc1(x)\n",
    "        return squashed_diagonal_gaussian_head(x)\n",
    "\n",
    "policy = PolicyFunction(obs_space.shape[1], obs_space.shape[2], action_size)\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)"
   ]
  },
  {
   "source": [
    "# print(obs_space.sample().shape)\n",
    "# policy(torch.from_numpy(obs_space.sample()).unsqueeze(0))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23104\n23104\n"
     ]
    }
   ],
   "source": [
    "class QFunction(nn.Module):\n",
    "    def __init__(self, width, height, action_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # RGB Image tensor as input\n",
    "        self.selectTrackFeatures, self.linear_input_size = make_conv2d_layer(width, height)\n",
    "        self.fc1 = make_linear_layer(self.linear_input_size+action_size, 1)\n",
    "    \n",
    "    def forward(self, state_and_action):\n",
    "        state = self.selectTrackFeatures(state_and_action[0])\n",
    "        x = torch.cat((state, state_and_action[1]), dim=-1)\n",
    "        return self.fc1(x)\n",
    "\n",
    "q_func1 = QFunction(obs_space.shape[1], obs_space.shape[2], action_size)\n",
    "q_func2 = QFunction(obs_space.shape[1], obs_space.shape[2], action_size)\n",
    "q_func1_optimizer = torch.optim.Adam(q_func1.parameters(), lr=3e-4)\n",
    "q_func2_optimizer = torch.optim.Adam(q_func2.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "source": [
    "# obs = torch.from_numpy(obs_space.sample()).unsqueeze(0)\n",
    "# print(obs.shape)\n",
    "# action = torch.from_numpy(action_space.sample()).unsqueeze(0)\n",
    "# print(action.shape)\n",
    "# q_func1((obs, action))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbuf = pfrl.replay_buffers.ReplayBuffer(10 ** 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def burnin_action_func():\n",
    "    \"\"\"Select random actions until model is updated one or more times.\"\"\"\n",
    "    return np.random.uniform(action_space.low, action_space.high).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "replay_start_size = 10000\n",
    "gpu = 0\n",
    "batch_size = 256\n",
    "entropy_target = -action_size\n",
    "temperature_optimizer_lr = 3e-4\n",
    "\n",
    "agent = pfrl.agents.SoftActorCritic(\n",
    "    policy,\n",
    "    q_func1,\n",
    "    q_func2,\n",
    "    policy_optimizer,\n",
    "    q_func1_optimizer,\n",
    "    q_func2_optimizer,\n",
    "    rbuf,\n",
    "    gamma=gamma,\n",
    "    replay_start_size=replay_start_size,\n",
    "    gpu=gpu,\n",
    "    minibatch_size=batch_size,\n",
    "    burnin_action_func=burnin_action_func,\n",
    "    entropy_target=entropy_target,\n",
    "    temperature_optimizer_lr=temperature_optimizer_lr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Track generation: 946..1189 -> 243-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 995..1254 -> 259-tiles track\n",
      "Track generation: 1105..1385 -> 280-tiles track\n",
      "Track generation: 1147..1437 -> 290-tiles track\n",
      "Track generation: 1155..1448 -> 293-tiles track\n",
      "Track generation: 1076..1349 -> 273-tiles track\n",
      "Track generation: 1091..1371 -> 280-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1014..1279 -> 265-tiles track\n",
      "Track generation: 1250..1574 -> 324-tiles track\n",
      "Track generation: 1139..1428 -> 289-tiles track\n",
      "Track generation: 1239..1557 -> 318-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1202..1507 -> 305-tiles track\n",
      "Track generation: 1278..1604 -> 326-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1171..1468 -> 297-tiles track\n",
      "episode: 10 R: -83.10810810810761 \n",
      "statistics: [('average_q1', 4.6859283), ('average_q2', 4.6744957), ('average_q_func1_loss', 0.27161712139844896), ('average_q_func2_loss', 0.3174185274541378), ('n_updates', 11979), ('average_entropy', 1.5182031), ('temperature', 0.06403522193431854)]\n",
      "Track generation: 1096..1376 -> 280-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1074..1346 -> 272-tiles track\n",
      "Track generation: 1108..1389 -> 281-tiles track\n",
      "Track generation: 1195..1498 -> 303-tiles track\n",
      "Track generation: 1079..1361 -> 282-tiles track\n",
      "Track generation: 1095..1376 -> 281-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1267..1588 -> 321-tiles track\n",
      "Track generation: 985..1235 -> 250-tiles track\n",
      "Track generation: 1427..1788 -> 361-tiles track\n",
      "Track generation: 1306..1636 -> 330-tiles track\n",
      "Track generation: 1019..1278 -> 259-tiles track\n",
      "Track generation: 1128..1414 -> 286-tiles track\n",
      "episode: 20 R: -75.43859649122804 \n",
      "statistics: [('average_q1', 0.67567754), ('average_q2', 0.6142168), ('average_q_func1_loss', 0.3746873998641968), ('average_q_func2_loss', 0.38760059207677844), ('n_updates', 13979), ('average_entropy', 1.2170625), ('temperature', 0.0415923148393631)]\n",
      "Track generation: 1178..1466 -> 288-tiles track\n",
      "Track generation: 1205..1510 -> 305-tiles track\n",
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1141..1430 -> 289-tiles track\n",
      "Track generation: 1236..1549 -> 313-tiles track\n",
      "Track generation: 1131..1418 -> 287-tiles track\n",
      "Track generation: 1296..1624 -> 328-tiles track\n",
      "Track generation: 1231..1543 -> 312-tiles track\n",
      "Track generation: 1131..1423 -> 292-tiles track\n",
      "episode: 30 R: -41.5807560137465 \n",
      "statistics: [('average_q1', -2.946353), ('average_q2', -2.9391215), ('average_q_func1_loss', 0.4778620453178883), ('average_q_func2_loss', 0.5255161002278328), ('n_updates', 15979), ('average_entropy', 0.8887383), ('temperature', 0.02714875340461731)]\n",
      "Track generation: 1164..1459 -> 295-tiles track\n",
      "Track generation: 963..1208 -> 245-tiles track\n",
      "Track generation: 1187..1488 -> 301-tiles track\n",
      "Track generation: 1164..1459 -> 295-tiles track\n",
      "Track generation: 1331..1668 -> 337-tiles track\n",
      "Track generation: 1040..1304 -> 264-tiles track\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "Track generation: 959..1207 -> 248-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1175..1473 -> 298-tiles track\n",
      "Track generation: 1148..1439 -> 291-tiles track\n",
      "Track generation: 1205..1511 -> 306-tiles track\n",
      "episode: 40 R: 11.475409836065491 \n",
      "statistics: [('average_q1', -6.8769755), ('average_q2', -6.8913436), ('average_q_func1_loss', 0.4851079152524471), ('average_q_func2_loss', 0.5925107002258301), ('n_updates', 17979), ('average_entropy', 0.54998666), ('temperature', 0.017784330993890762)]\n",
      "Track generation: 1227..1538 -> 311-tiles track\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "Track generation: 1017..1275 -> 258-tiles track\n",
      "Track generation: 1189..1490 -> 301-tiles track\n",
      "Track generation: 993..1245 -> 252-tiles track\n",
      "Track generation: 1190..1491 -> 301-tiles track\n",
      "Track generation: 1053..1324 -> 271-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1303..1640 -> 337-tiles track\n",
      "Track generation: 1215..1523 -> 308-tiles track\n",
      "Track generation: 1009..1265 -> 256-tiles track\n",
      "Track generation: 1184..1484 -> 300-tiles track\n",
      "episode: 50 R: -43.14381270903068 \n",
      "statistics: [('average_q1', -11.177045), ('average_q2', -11.227), ('average_q_func1_loss', 0.701604252755642), ('average_q_func2_loss', 0.6926707765460014), ('n_updates', 19979), ('average_entropy', 0.01400317), ('temperature', 0.011742062866687775)]\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 50\n",
    "max_episode_len = 1000\n",
    "\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while True:\n",
    "        # Uncomment to watch the behavior in a GUI window\n",
    "        # env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        reset = t == max_episode_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        # print(f\"action: {action}, reward: {reward}\")\n",
    "        if done or reset:\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i, 'R:', R, '\\nstatistics:', agent.get_statistics())\n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "source": [
    "### Random"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Track generation: 1171..1468 -> 297-tiles track\n",
      "R: -83.10810810810761\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "frames = []\n",
    "obs = env.reset()\n",
    "total_r = 0\n",
    "\n",
    "while not done:\n",
    "    action = -1+np.random.rand(3)*2\n",
    "    obs, r, done, info = env.step(action)\n",
    "    total_r += r\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "\n",
    "print('R:', total_r)\n",
    "display_frames_as_gif(frames, 'CarRacing_Random.gif')"
   ]
  },
  {
   "source": [
    "### Trained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Track generation: 1105..1393 -> 288-tiles track\n",
      "R: 35.888501742161026\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "frames = []\n",
    "obs = env.reset()\n",
    "total_r = 0\n",
    "\n",
    "with agent.eval_mode():\n",
    "    while not done:\n",
    "        action = agent.act(obs)\n",
    "        obs, r, done, info = env.step(action)\n",
    "        total_r += r\n",
    "        agent.observe(obs, r, done, reset)\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "print('R:', total_r)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}