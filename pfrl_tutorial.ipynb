{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd0b96d43ddc357ac6d8b0a4722a84f6c15c14d8271114e14b3c53cab3640efc3f5",
   "display_name": "Python 3.8.6 64-bit ('.venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "b96d43ddc357ac6d8b0a4722a84f6c15c14d8271114e14b3c53cab3640efc3f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "import torch\n",
    "import torch.nn\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\naction space: Discrete(2)\ninitial observation: [ 0.03481615 -0.0239378  -0.04042435 -0.02445456]\nnext observation: [ 0.03433739 -0.21845741 -0.04091344  0.25520497]\nreward: 1.0\ndone: False\ninfo: {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "obs = env.reset()\n",
    "print('initial observation:', obs)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "print('next observation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)\n",
    "\n",
    "# Uncomment to open a GUI window rendering the current state of the environment\n",
    "# env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(obs_size, 50)\n",
    "        self.l2 = torch.nn.Linear(50, 50)\n",
    "        self.l3 = torch.nn.Linear(50, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = torch.nn.functional.relu(self.l1(h))\n",
    "        h = torch.nn.functional.relu(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        return pfrl.action_value.DiscreteActionValue(h)\n",
    "\n",
    "obs_size = env.observation_space.low.size\n",
    "n_actions = env.action_space.n\n",
    "q_func = QFunction(obs_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "optimizer = torch.optim.Adam(q_func.parameters(), eps=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "explorer = pfrl.explorers.ConstantEpsilonGreedy(epsilon=0.3, random_action_func=env.action_space.sample)\n",
    "\n",
    "replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10**6)\n",
    "\n",
    "phi = lambda x: x.astype(np.float32, copy=False)\n",
    "\n",
    "gpu = -1\n",
    "\n",
    "agent = pfrl.agents.DoubleDQN(\n",
    "    q_func,\n",
    "    optimizer,\n",
    "    replay_buffer,\n",
    "    gamma,\n",
    "    explorer,\n",
    "    replay_start_size=500,\n",
    "    update_interval=1,\n",
    "    target_update_interval=100,\n",
    "    phi=phi,\n",
    "    gpu=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "episode: 10 R: 24.0\n",
      "episode: 20 R: 20.0\n",
      "episode: 30 R: 21.0\n",
      "episode: 40 R: 15.0\n",
      "episode: 50 R: 12.0\n",
      "statics: [('average_q', 2.7332926), ('average_loss', 0.11139749635942281), ('cumulative_steps', 792), ('n_updates', 293), ('rlen', 792)]\n",
      "episode: 60 R: 16.0\n",
      "episode: 70 R: 9.0\n",
      "episode: 80 R: 9.0\n",
      "episode: 90 R: 10.0\n",
      "episode: 100 R: 12.0\n",
      "statics: [('average_q', 6.223807), ('average_loss', 0.333179437154904), ('cumulative_steps', 1367), ('n_updates', 868), ('rlen', 1367)]\n",
      "episode: 110 R: 8.0\n",
      "episode: 120 R: 19.0\n",
      "episode: 130 R: 14.0\n",
      "episode: 140 R: 150.0\n",
      "episode: 150 R: 59.0\n",
      "statics: [('average_q', 9.069472), ('average_loss', 0.3329277034010738), ('cumulative_steps', 2636), ('n_updates', 2137), ('rlen', 2636)]\n",
      "episode: 160 R: 44.0\n",
      "episode: 170 R: 80.0\n",
      "episode: 180 R: 121.0\n",
      "episode: 190 R: 200.0\n",
      "episode: 200 R: 169.0\n",
      "statics: [('average_q', 9.914565), ('average_loss', 0.12350330051500351), ('cumulative_steps', 8073), ('n_updates', 7574), ('rlen', 8073)]\n",
      "episode: 210 R: 145.0\n",
      "episode: 220 R: 181.0\n",
      "episode: 230 R: 28.0\n",
      "episode: 240 R: 200.0\n",
      "episode: 250 R: 71.0\n",
      "statics: [('average_q', 10.141422), ('average_loss', 0.08717064352123999), ('cumulative_steps', 15697), ('n_updates', 15198), ('rlen', 15697)]\n",
      "episode: 260 R: 193.0\n",
      "episode: 270 R: 184.0\n",
      "episode: 280 R: 198.0\n",
      "episode: 290 R: 200.0\n",
      "episode: 300 R: 16.0\n",
      "statics: [('average_q', 9.963043), ('average_loss', 0.0571737344074063), ('cumulative_steps', 23019), ('n_updates', 22520), ('rlen', 23019)]\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 300\n",
    "max_episode_len = 200\n",
    "for i in range(1, n_episodes+1):\n",
    "    obs = env.reset()\n",
    "    R = 0\n",
    "    t = 0\n",
    "    while True:\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        reset = t == max_episode_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        if done or reset:\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i, 'R:', R)\n",
    "    if i % 50 == 0:\n",
    "        print('statics:', agent.get_statistics())\n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "evaluatin episode: 0 R: 200.0\n",
      "evaluatin episode: 1 R: 200.0\n",
      "evaluatin episode: 2 R: 185.0\n",
      "evaluatin episode: 3 R: 184.0\n",
      "evaluatin episode: 4 R: 184.0\n",
      "evaluatin episode: 5 R: 194.0\n",
      "evaluatin episode: 6 R: 168.0\n",
      "evaluatin episode: 7 R: 168.0\n",
      "evaluatin episode: 8 R: 200.0\n",
      "evaluatin episode: 9 R: 200.0\n"
     ]
    }
   ],
   "source": [
    "with agent.eval_mode():\n",
    "    for i in range(10):\n",
    "        obs = env.reset()\n",
    "        R = 0\n",
    "        t = 0\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = agent.act(obs)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            R += r\n",
    "            t += 1\n",
    "            reset = t == 200\n",
    "            agent.observe(obs, r, done, reset)\n",
    "            if done or reset:\n",
    "                break\n",
    "        print('evaluatin episode:', i, 'R:', R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "source": [
    "## SAC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "import pfrl\n",
    "import torch\n",
    "import torch.nn\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "obs: Box(-8.0, 8.0, (3,), float32)\nacitons: [-0.09511738]\nobsavation: [-0.93061906  0.3659893   1.27269098]\nreward: -7.401462890176967\ndone: False\ninfo: {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "print(f'obs: {env.observation_space}')\n",
    "obs_init = env.reset()\n",
    "\n",
    "ac = env.action_space.sample()\n",
    "print('acitons:', ac)\n",
    "obs, r, done, info = env.step(ac)\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(np.hstack([obs_init, obs]))\n",
    "print('obsavation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)\n"
   ]
  },
  {
   "source": [
    "SACは状態と行動値を受け取って行動価値を返すQ関数と\n",
    "状態を受け取って行動を返す方策関数からなる Actor-Criticである。\n",
    "方策関数は平均と標準偏差をネットワークで予測し、ガウス分布を返す。\n",
    "Q関数は状態と行動をconcatしてMLPに通して価値を返す。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions as dists\n",
    "\n",
    "class Qfunc(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(obs_size + n_actions, 50)\n",
    "        self.l2 = torch.nn.Linear(50, 50)\n",
    "        self.l3 = torch.nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        state, action = x\n",
    "        h = torch.cat([state, action], 1)\n",
    "        h = torch.nn.functional.relu(self.l1(h))\n",
    "        h = torch.nn.functional.relu(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        return h\n",
    "\n",
    "    def __init__(self, obs_size, n_actions, log_std_max=3, log_std_min=-15):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(obs_size, 50)\n",
    "        self.l2 = torch.nn.Linear(50, 50)\n",
    "        self.mean = torch.nn.Linear(50, n_actions)\n",
    "        self.log_std = torch.nn.Linear(50, n_actions)\n",
    "        self.log_std_max = log_std_max\n",
    "        self.log_std_min = log_std_min\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = torch.nn.functional.relu(self.l1(h))\n",
    "        h = torch.nn.functional.relu(self.l2(h))\n",
    "        mean = self.mean(h)\n",
    "        log_std = self.log_std(h)\n",
    "        log_std = torch.clamp(log_std, min=self.log_std_min, max=self.log_std_max)\n",
    "        dist = dists.Normal(mean, log_std.exp())\n",
    "        return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-3f0584cab758>, line 23)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-3f0584cab758>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    batch_terminal = batch[]\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pfnet/pfrl/blob/master/pfrl/agents/soft_actor_critic.py\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ModSAC(pfrl.agents.SoftActorCritic):\n",
    "\n",
    "    def _batch_act_train(self, batch_obs):\n",
    "        if self.burnin_action_func is not None and self.n_policy_updates == 0:\n",
    "            batch_action = [self.burnin_action_func() for _ in range(len(batch_obs))]\n",
    "        else:\n",
    "            batch_action = self.batch_select_greedy_action(batch_obs, deterministic=self.act_deterministically)\n",
    "        \n",
    "        self.batch_last_obs = list(batch_obs)\n",
    "        self.batch_last_action = list(batch_action)\n",
    "\n",
    "        return batch_action\n",
    "\n",
    "    def update_q_func(self, batch):\n",
    "        \"\"\"Compute loss for a given Q-function.\"\"\"\n",
    "\n",
    "        batch_next_state = batch[\"next_state\"]\n",
    "        batch_rewards = batch[\"reward\"]\n",
    "        batch_terminal = batch[\"is_state_terminal\"]\n",
    "        batch_actions = batch[\"action\"]\n",
    "        batch_discount = batch[\"discount\"]\n",
    "\n",
    "        with torch.no_grad(), pfrl.utils.evaluating(self.policy), pfrl.utils.evaluating(self.target_q_func1), pfrl.utils.evaluating(self.target_q_func2):\n",
    "            next_action_distrib = self.policy(batch_next_state)\n",
    "            next_actions = next_action_distrib.sample()\n",
    "            next_log_prob = next_action_distrib.log_prob(next_actions)\n",
    "            next_q1 = self.target_q_func1((batch_next_state, next_actions))\n",
    "            next_q2 = self.target_q_func2((batch_next_state, next_actions))\n",
    "            next_q = torch.min(next_q1, next_q2)\n",
    "            entropy_term = self.temperature * next_log_prob\n",
    "            assert next_q.shape == entropy_term.shape\n",
    "\n",
    "            target_q = batch_rewards + batch_discount * (1.0 - batch_terminal) * torch.flatten(next_q - entropy_term)\n",
    "\n",
    "        predict_q1 = torch.flatten(self.q_func1((batch_state, batch_actions)))\n",
    "        predict_q2 = torch.flatten(self.q_func2((batch_state, batch_actions)))\n",
    "\n",
    "        print(f'=====\\n{batch_terminal}') \n",
    "\n",
    "        loss1 = 0.5 * F.mse_loss(target_q, predict_q1)\n",
    "        loos2 = 0.5 * F.mse_loss(target_q, predict_q2)\n",
    "\n",
    "        self.q1_record.extend(predict_q1.detach().cpu().numpy())\n",
    "        self.q2_record.extend(predict_q2.detach().cpu().numpy())\n",
    "        self.q_func1_loss_record.append(float(loss1))\n",
    "        self.q_func2_loss_record.append(float(loss2))\n",
    "\n",
    "        self.q_func1_optimizer.zero_grad()\n",
    "        loss1.backward()\n",
    "        if self.max_grad_norm is not None:\n",
    "            clip_l2_grad_norm_(self.q_func1.parameters(), self.max_grad_norm)\n",
    "        self.q_func1_optimizer.step()\n",
    "\n",
    "        self.q_func2_optimizer.zero_grad()\n",
    "        loss2.backward()\n",
    "        if self.max_grad_norm is not None:\n",
    "            clip_l2_grad_norm_(self.q_func2.parameters(), self.max_grad_norm)\n",
    "        self.q_func2_optimizer.step()\n",
    "\n",
    "        \n",
    "        \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 500\n",
    "gamma = 0.99\n",
    "phi = lambda x: x.astype(np.float32, copy=False)\n",
    "gpu = 1\n",
    "num_runs = 3\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "res_dfs = []\n",
    "results = []\n",
    "for run in range(num_runs):\n",
    "    env.seed(run)\n",
    "    set_seed(run)\n",
    "    policy_func = pfrl.policy."
   ]
  }
 ]
}